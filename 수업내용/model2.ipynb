{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3caaa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "520015ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "510cc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터\n",
    "x = torch.linspace(-math.pi, math.pi, 1000)\n",
    "# -π부터 π까지의 구간을 1000개의 점으로 나눈 1차원 텐서를 만드는 코드\n",
    "\n",
    "\n",
    "# torch.linspace(start, end, steps)\n",
    "# start: 시작 값\n",
    "# end: 끝 값\n",
    "# steps: 만들고 싶은 값의 개수\n",
    "# 즉, start에서 end까지를 균등하게 나누어 steps개의 값을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4e76df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1afd0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차 다항식\n",
    "# f(x) = a*x**3 + b*x**2 + c*x + d\n",
    "# 여기서 다항식의 계수들 a, b, c, d의 값을 구하는게 머신러닝의 회귀 방식\n",
    "\n",
    "# 초기화\n",
    "a,b,c,d = torch.randn(()),torch.randn(()),torch.randn(()),torch.randn(()) # (()) 이렇게 해야되는 이유 ^\n",
    "y_random = a*x**3 + b*x**2 + c*x + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe412fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "# 1.실제값 시각화\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.title('y true')\n",
    "# plt.plot(x,y)\n",
    "# 2 임의의 가중치로 만든 예측용 값으로 시각화\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.title('y random')\n",
    "# plt.plot(x,y_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fcea493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 파라미터 정의\n",
    "# learning_rate = 1e-6 # 0.0000001 \n",
    "#                      # 학습률 지정 : 적절한 가중치를 찾을 때 어느 간격으로 할지\n",
    "# total_loss = []\n",
    "# for epoch in range(2000): # 전체 학습과정 횟수 지정 : 2000\n",
    "#     # forward\n",
    "#     y_pred = a*x**3 + b*x**2 + c*x + d # 모델\n",
    "#     # 손실 정의 MSE(평균 제곱오차) # 오차를 제곱음수를 방지해서 크기만 보도록\n",
    "#     # 오차가 큰 값에 더 큰 패널티 부여\n",
    "#         # ^ 어떤 패널티? 제곱을 하니까 오차가 클수록 값이 훨씬 커짐 → 이게 바로 큰 오차를 “더 중요하게” 만드는 효과\n",
    "#             # 오차      제곱   더함   저장 ^ 왜 평균을 내는지 ^ 모델 자체의 전반적인 성능 평가를 하기위해서 ^\n",
    "#                                         # ^ 왜 저장하는지 ^ \n",
    "#     loss = (y_pred - y).pow(2).mean().item() # 텐서에 저장된 숫자값만 추출(그래프 추적을 피함) ^\n",
    "#     total_loss.append(loss)\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'epoch {epoch+1} : loss : {loss}')\n",
    "#     # 역전파 계산 loss 줄이기 위해서 a,b,c,d를 확인할 필요가 있다. 그래야 어느 방향으로 움직여야 할지 계산이 가능하다.\n",
    "#     # 미분\n",
    "#     #               2 * 오차 ^ 왜 2 * 오차? MSE를 적용한 값에 대한 미분값, 손실이 줄거나 증가하는 방향, 기울기의 부호를 확인하기위해서\n",
    "#     #                                                                   y_pred > y -> 기울기가 양수 -> 손실을 줄이려면 y_pred를 줄여야한다.\n",
    "#     #                                                                   y_pred < y -> 기울기가 음수 -> 손실을 줄이려면 y_pred를 늘려야한다.\n",
    "#     grad_y_pred = 2.0*(y_pred-y) # 오차 mse 적용한 값을 미분\n",
    "#     grad_a = (grad_y_pred*x**3).sum() # ^ 왜 .sum() ?\n",
    "#     grad_b = (grad_y_pred*x**2).sum()\n",
    "#     grad_c = (grad_y_pred*x).sum()\n",
    "#     grad_d = (grad_y_pred).sum()\n",
    "\n",
    "#     # 가중치를 업데이트 \n",
    "#     a -= learning_rate*grad_a \n",
    "#     b -= learning_rate*grad_b\n",
    "#     c -= learning_rate*grad_c\n",
    "#     d -= learning_rate*grad_d\n",
    "\n",
    "#     plt.subplot(3,1,1)\n",
    "#     plt.plot(x,y)\n",
    "#     plt.subplot(3,1,2)\n",
    "#     plt.plot(x,y_random)\n",
    "#     plt.subplot(3,1,3)\n",
    "#     plt.plot(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f6e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라메터 정의\n",
    "learning_rate = 1e-6\n",
    "total_loss = []\n",
    "for epoch in range(2000):\n",
    "    # forward\n",
    "    y_pred = a*x**3 + b*x**2 + c*x + d  # 모델\n",
    "    # 손실 정의  MSE(평균제곱오차)  # 오차를 제곱 음수를 방지해서 크기만 보도록\n",
    "    # 오차가 큰 값에 더 큰 패널티 부여\n",
    "    loss = torch.sqrt((y_pred - y)**2).mean().item()  # 텐서에 저장된 숫자값만 추출 (그래프 추적을 피함)\n",
    "    total_loss.append(loss)\n",
    "    # 역전파 계산(BackPropagation) loss 줄이기 위해서  a,b,c,d 어느 방향으로 움직여야 할지 계산\n",
    "    # 미분\n",
    "    grad_y_pred = 2.0*(y_pred - y)  #기울기의 미분값\n",
    "    grad_a = 2.0*(grad_y_pred*x**3).sum()\n",
    "    grad_b = 2.0*(grad_y_pred*x**2).sum()\n",
    "    grad_c = 2.0*(grad_y_pred*x).sum()\n",
    "    grad_d = 2.0*(grad_y_pred).sum()\n",
    "    # 가중치 업데이트\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "# plt.subplot(3,1,1)\n",
    "# plt.plot(x,y)\n",
    "# plt.subplot(3,1,2)\n",
    "# plt.plot(x,y_random)\n",
    "# plt.subplot(3,1,3)\n",
    "# plt.plot(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cf844a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(2000), total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380529c",
   "metadata": {},
   "source": [
    "```\n",
    "D-loss / D-a\n",
    "D-loss / D-b\n",
    "D-loss / D-c\n",
    "D-loss / D-d\n",
    "\n",
    "loss 는 파라미터들과 직접 연관이 없다.\n",
    "다만, 다음과 같은 연쇄구조를 가진다.\n",
    "\n",
    "a,b,c,d -> ypred -> loss\n",
    "loss = (ypred - ytrue)\n",
    "\n",
    "D-loss / D-a = D-ypred /D-a * D-loss / D-pred\n",
    "\n",
    "D-loss / D-ypred = 2*(ypred-y)\n",
    "    # (ypred-y)**2 의 미분한값\n",
    "\n",
    "D-ypred / D-a = x**3\n",
    "\n",
    "1. 예측 → MSE로 점수 확인\n",
    "2. 미분 → 어디로 움직여야 손실이 줄어드는지 확인\n",
    "3. 역전파 → 모든 파라미터에 적용\n",
    "4. 조금씩 업데이트 → 학습 완료\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b9d7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install pandas\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c1c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Playdata2\\AppData\\Local\\Temp\\ipykernel_17516\\1098118733.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    }
   ],
   "source": [
    "# 보스턴 집값 예측\n",
    "# from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cab38754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 : loss : 638.982177734375\n",
      "epoch : 21 : loss : 20.983261108398438\n",
      "epoch : 41 : loss : 19.153413772583008\n",
      "epoch : 61 : loss : 17.688077926635742\n",
      "epoch : 81 : loss : 16.54913330078125\n",
      "epoch : 101 : loss : 15.55953598022461\n",
      "epoch : 121 : loss : 14.692230224609375\n",
      "epoch : 141 : loss : 13.735527038574219\n",
      "epoch : 161 : loss : 12.848262786865234\n",
      "epoch : 181 : loss : 12.13191032409668\n"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "# 미분을 자동으로 계산\n",
    "# 계산된 미분을 optimizer를 통해 적용 : 최적화 알고리즘\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "# 모델 정의\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(data.shape[1] , 100),  # 13의 데이터를 입력받아서 결과를 100개 즉 100개를 예측\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)  # 100개의 예측결과를 받아서 최종 1개를 예측\n",
    ")\n",
    "\n",
    "# 하이퍼 파라메터 정의\n",
    "batch_size = 100\n",
    "learning_rate = 1e-03  # 0.001\n",
    "\n",
    "# 옵티마이져 정의\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "target = target.reshape(-1,1)\n",
    "# 학습루프\n",
    "for epoch in range(200):\n",
    "    for i in range(len(data) // batch_size):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        X = torch.FloatTensor(data[start:end])\n",
    "        y = torch.FloatTensor(target[start:end])\n",
    "\n",
    "        optim.zero_grad()  # 학습이후 다음 학습에 이전 가중치가 계산되는것을 방지\n",
    "        preds = model(X)  # 순전파.. forward  \n",
    "        loss = nn.MSELoss()(preds, y)    # preds, 정답 shape 같아야함 \n",
    "        loss.backward()  # 기울기 계산  각 파라메터의 그레이던트를 자동으로 계산   .grad 저장\n",
    "        optim.step()  # 옵티마이져가 .grad 정보를 사용해서 파라메터를 갱신\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'epoch : {epoch + 1} : loss : {loss.item()}') # 에포크의 가장 마지막 배치만 출력(평균)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24fea4",
   "metadata": {},
   "source": [
    "```\n",
    "순전파 : X -> model -> preds -> loss\n",
    "역전파 : dloss/lpreds 계산해서 각 선형층의 파라메터(가중치,편향)로 전파\n",
    "loss.backward() 호출되어서 pytorch autograd가 연쇄법칙(chain rule)을 이용해서 각 파라메터의 .grad를 채움\n",
    "optim.step() : .grad를 읽어서 파라메터를 갱신\n",
    "\n",
    "ReLU(활성화 함수)  :  max(0,z)\n",
    "z가 음수이면 0, 양수면 그대로 전달\n",
    "역전파 \n",
    "    - z >0  dReLU(z)/dz = 1\n",
    "    - z <=0 dReLU(z)/dz =  0\n",
    "    - 비선형을 제공해서 계산이 간단해진다\n",
    "    - 기울기 소실이 sigmoid, tanh 보다 유리\n",
    "    - 음수이면 0이니깐.. 일부 뉴런이 비활성화되어 표현이 희소해진다.  -> LeekyReLU\n",
    "\n",
    "Adam : 옵티마이져\n",
    "    - 모멘텀 과 스케일 조정 두가지 방법을 결합\n",
    "    - 적응적 학습률 : 파라메터별로 학습률을 조정 - > 초기값 안정적\n",
    "    - 빠른 수렴 : SGD 보다 빠름\n",
    "    - 튜닝이 거의 필요없음... 기본 파라메터가 lr = 0.001 도 좋은 성능\n",
    "    - 단점 : 과적합이 다른 최적화 모델에 비해 발생하기 쉬운구조\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d4ebb",
   "metadata": {},
   "source": [
    "```\n",
    "옵티마이져 : 모델 파라메터를 손실이 줄여들게 갱신하는 역활\n",
    "p_t : 현재파라메너\n",
    "n : 학습률\n",
    "gt : 현재 그레이디언트\n",
    "\n",
    "p_t+1 = p_t -n*gt\n",
    "단점 : \n",
    "    기울기가 들쑥날쑥(노이즈에  약함)\n",
    "    모든 파라메터가 같은 학습률을 적용(비효율적)\n",
    "    --> 모멘텀, 스케일 조정\n",
    "\n",
    "모멘템 Momentum\n",
    "    이전 기울기의 방향을 \"기억\" 해서 관성처럼 부드럽게 이동하는 기법\n",
    "    vi : 누적된 속도 또는 1차 모멘트\n",
    "    gt : 현재 그레이디언트\n",
    "    b1 : 모멘텀의 개수(보통 0.9)\n",
    "    n : 학습률\n",
    "    vt = b1vt-1 + (1-b1)gt\n",
    "        pt+1 = pt - n*vt\n",
    "시점\n",
    "    t : 1          단순히 g1을 따라감\n",
    "    t : 2          이전 속도 v1의 일부를 남겨서 새로운 방향에 더해\n",
    "    t -> inif      여러 스텝동안 방향이 일정하면 점점 가속되어 빠르게 수렴 \n",
    "기울기가 변해도 관성때문에 부드럽게 움직임\n",
    "\n",
    "스케일 조정\n",
    "    파라메터의 기울기의 크기를 고려해서 학습률을 자동으로 조정\n",
    "    과거 파라메터의 크기를 추적해서 큰 변동이 있었던 파라메터는 작게, 작은변동은 크게 이동\n",
    "\n",
    "아담 : 모멘텀 + 스케일 조정 하이브리드 방식\n",
    "    step 1 : 모멘텀\n",
    "    step 2 : 스케일 조정\n",
    "    step 3 : 바이서 보정  초기 스텝에서는 m,v = 0 보정\n",
    "    step 4 : 최종업데이트 : 모멘텀으로 얻은 방향 + 스케일 조정된 학습률 이동\n",
    "\n",
    "SGD                     단순히 그레이디언트                     진동심함, 느림\n",
    "Momentum                과거 방향의 관성 사용                   진동완화, 빠른수렴\n",
    "RMSProp / AdaGrad       그레이디언트 제곱의 평균으로 스케일조정   파라메터별 자동 학습률\n",
    "Adam                    모멘텀 + RMSProm 결합                   기본설정도 훌륭함\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a7afb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((506, 1), (506, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 성능 평가  : 예측\n",
    "print(data.shape , target.shape)\n",
    "predict = model(torch.FloatTensor(data)).detach().numpy()\n",
    "predict.shape,   target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a7099ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672836102114631"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(target, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b10863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적절한 머신러닝 알고리즘으로 비교 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a93e7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8737432209837027\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "model_2 = RandomForestRegressor()\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,target,random_state=42,train_size=0.8)\n",
    "model_2.fit(x_train,y_train)\n",
    "predict_2 = model_2.predict(x_test)\n",
    "print(r2_score(y_test,predict_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef71bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from requests->kagglehub) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata2\\miniconda3\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79b9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n",
      "C:\\Users\\Playdata2\\.cache\\kagglehub\\datasets\\aashita\\nyt-comments\\versions\\13\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1cdf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "1      NaN  58def3237c459f24986d7c84               823   \n",
       "2      NaN  58def9f57c459f24986d7c90               575   \n",
       "3      NaN  58defd317c459f24986d7c95              1374   \n",
       "4      NaN  58df09b77c459f24986d7ca7               708   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                      By GAIL COLLINS      article   \n",
       "2               By THE EDITORIAL BOARD      article   \n",
       "3                    By MICHAEL POWELL      article   \n",
       "4                         By DEB AMLEN      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                  And Now,  the Dreaded Trump Curse   \n",
       "2              Venezuela’s Descent Into Dictatorship   \n",
       "3              Stain Permeates Basketball Blue Blood   \n",
       "4                          Taking Things for Granted   \n",
       "\n",
       "                                            keywords  multimedia    newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1  ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2  ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3  ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                              ['Crossword Puzzles']           3      Games   \n",
       "\n",
       "   printPage              pubDate         sectionName  \\\n",
       "0          2  2017-04-01 00:15:41             Unknown   \n",
       "1         23  2017-04-01 00:23:58             Unknown   \n",
       "2         22  2017-04-01 00:53:06             Unknown   \n",
       "3          1  2017-04-01 01:06:52  College Basketball   \n",
       "4          0  2017-04-01 02:00:14             Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "1                  Meet the gang from under the bus.  The New York Times   \n",
       "2  A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3  For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4  In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1          Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2      Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3           News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4           News  https://www.nytimes.com/2017/03/31/crosswords/...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4b800",
   "metadata": {},
   "source": [
    "##### LSTM\n",
    "- 입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 ___ 을 잘합니다.\"\n",
    "- 일반신경망 : 공부  ( 파이썬 정보가 희석... 잊어)\n",
    "- LSTM : 프로그래밍(오래된 정보도 기억)\n",
    "\n",
    "- 핵심 키워드\n",
    "    - 장기기억 : 중요한 정보는 오래기억\n",
    "    - 단기기억 : 불필요한 정보는 버림\n",
    "    - 순서이해 : 시간순서 이해\n",
    "\n",
    "##### 3개의 Gate를 통해 정보의 흐름을 제어\n",
    " - LSTM 셀 (한 시점 t)\n",
    "    - 입력 : $x_t$ (현재데이터)\n",
    "    - 이전 은닉상태 : $h_{t-1}$\n",
    "    - 이전 셀 상태 : $c_{t-1}$\n",
    "\n",
    "    --> forget gate --> input date   --> output gate\n",
    "        잊을데이터        추가할 데이터     출력할 데이터\n",
    "    \n",
    "    출력 $h_t$ ,  $c_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702adbf",
   "metadata": {},
   "source": [
    "Forget Gate(잊음 관문)\n",
    "\n",
    "\n",
    "$f_t$ = $s(w_f . [h_{t-1}, x_t ] + b_f )$\n",
    "\n",
    "s : sigmoid함수(0~1)\n",
    "\n",
    "이전 셀상태 : [1.5, -0.3, 2.1]\n",
    "현재입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1,0.05,0.9]\n",
    "\n",
    "결과 : [ 1.5*0.1, -0.3*0.05, 2.1*0.9  ]  =  [0.15, -0.015, 1.89]\n",
    "\n",
    "첫 2개는 버리고 3번째는 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937c4b4",
   "metadata": {},
   "source": [
    "##### input gate : 입력\n",
    "- 첫 번째는 70%받고 두번째는 30% 받음\n",
    "\n",
    "##### Cell State 업데이트\n",
    " - 이전기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84d41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455d6281",
   "metadata": {},
   "source": [
    "input 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4e0a9",
   "metadata": {},
   "source": [
    "시점 t에서:\n",
    "- $x_t$      : 현재 입력 데이터 (벡터)\n",
    "- $h_{t-1}$  : 이전 시점의 은닉 상태 (벡터)\n",
    "- $C_{t-1}$  : 이전 시점의 셀 상태 (벡터) ← 장기 기억!\n",
    "- $W_*$, $U_*$ : 가중치 행렬\n",
    "- $b_*$      : 편향 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa5e5b",
   "metadata": {},
   "source": [
    "- $x_t$ = [0.2, -0.5, 0.8]      (3개 입력 피처)\n",
    "- $h_{t-1}$ = [0.1, 0.3, -0.2, 0.5]  (4개 은닉)\n",
    "- $C_{t-1}$ = [0.4, -0.1, 0.6, 0.2]  (4개 셀 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009bbb3",
   "metadata": {},
   "source": [
    "Forget Gate ( $f_t$ ) - 어제 기억을 얼마나 유지할까\n",
    "- $f_t$ = $σ( W_f · [h_{t-1}, x_t] + b_f )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7bac5",
   "metadata": {},
   "source": [
    "1단계: $h_{t-1}$ 과 $x_t$ 를 연결 (concatenate)\n",
    "\n",
    "   $[ h_{t-1}, x_t]$ = [0.1,    0.3,    -0.2,    0.5,    0.2,    -0.5,    0.8]\n",
    "\n",
    "               $h_{t-1}$    $x_t$\n",
    "\n",
    "                        4개              3개\n",
    "\n",
    "                               ↓\n",
    "\n",
    "                           총 7개 벡터\n",
    "\n",
    "2단계: 가중치 행렬 곱하기\n",
    "   $W_f · [h_{t-1}, x_t]$\n",
    "   \n",
    "   $W_f$ 는 크기: (4, 7) 행렬\n",
    "   (왜 (4, 7)? → 은닉 크기 4개, 입력 7개)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "3단계: 편향 더하기\n",
    "   + $b_f$  (크기: 4)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "4단계: Sigmoid 함수 적용\n",
    "   $σ(x)$ = $\\frac{1}{1 + e^{-x}}$ \n",
    "   \n",
    "   이 함수는 모든 값을 0~1 사이로 변환!\n",
    "   \n",
    "   $f_t = [0.3, 0.8, 0.1, 0.9]$\n",
    "   \n",
    "   의미:\n",
    "   - 첫 번째 셀 상태: 30% 유지 (70% 잊음)\n",
    "   - 두 번째 셀 상태: 80% 유지 (20% 잊음)\n",
    "   - 세 번째 셀 상태: 10% 유지 (90% 잊음)\n",
    "   - 네 번째 셀 상태: 90% 유지 (10% 잊음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581a9d7",
   "metadata": {},
   "source": [
    "#### 학습용 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34890fc4",
   "metadata": {},
   "source": [
    "##### 데이터 전처리\n",
    "- 문장단위로 분류\n",
    "- 특수문자 제거\n",
    "- 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef2e115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if 'Articles' in path] # ^\n",
    "# headline 정보만 추출\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "# 특수문자 제거\n",
    "import string\n",
    "string.punctuation # ^ 파이썬의 특수문자 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7ecd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finding': 0,\n",
       " 'an': 1,\n",
       " 'expansive': 2,\n",
       " 'view': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'forgotten': 6,\n",
       " 'people': 7,\n",
       " 'in': 8,\n",
       " 'niger': 9,\n",
       " 'and': 10,\n",
       " 'now,': 11,\n",
       " 'the': 12,\n",
       " 'dreaded': 13,\n",
       " 'trump': 14,\n",
       " 'curse': 15,\n",
       " 'venezuela’s': 16,\n",
       " 'descent': 17,\n",
       " 'into': 18,\n",
       " 'dictatorship': 19,\n",
       " 'stain': 20,\n",
       " 'permeates': 21,\n",
       " 'basketball': 22,\n",
       " 'blue': 23,\n",
       " 'blood': 24,\n",
       " 'taking': 25,\n",
       " 'things': 26,\n",
       " 'for': 27,\n",
       " 'granted': 28,\n",
       " 'caged': 29,\n",
       " 'beast': 30,\n",
       " 'awakens': 31,\n",
       " 'ever-unfolding': 32,\n",
       " 'story': 33,\n",
       " 'o’reilly': 34,\n",
       " 'thrives': 35,\n",
       " 'as': 36,\n",
       " 'settlements': 37,\n",
       " 'add': 38,\n",
       " 'up': 39,\n",
       " 'mouse': 40,\n",
       " 'infestation': 41,\n",
       " 'divide': 42,\n",
       " 'g.o.p.': 43,\n",
       " 'now': 44,\n",
       " 'threatens': 45,\n",
       " 'tax': 46,\n",
       " 'plan': 47,\n",
       " 'variety': 48,\n",
       " 'puzzle:': 49,\n",
       " 'acrostic': 50,\n",
       " 'they': 51,\n",
       " 'can': 52,\n",
       " 'hit': 53,\n",
       " 'ball': 54,\n",
       " '400': 55,\n",
       " 'feet.': 56,\n",
       " 'but': 57,\n",
       " 'play': 58,\n",
       " 'catch?': 59,\n",
       " 'that’s': 60,\n",
       " 'tricky.': 61,\n",
       " 'country,': 62,\n",
       " 'shock': 63,\n",
       " 'at': 64,\n",
       " 'budget': 65,\n",
       " 'cuts': 66,\n",
       " 'why': 67,\n",
       " 'is': 68,\n",
       " 'this': 69,\n",
       " 'hate': 70,\n",
       " 'different': 71,\n",
       " 'from': 72,\n",
       " 'all': 73,\n",
       " 'other': 74,\n",
       " 'hate?': 75,\n",
       " 'pick': 76,\n",
       " 'your': 77,\n",
       " 'favorite': 78,\n",
       " 'ethical': 79,\n",
       " 'offender': 80,\n",
       " 'my': 81,\n",
       " 'son’s': 82,\n",
       " 'growing': 83,\n",
       " 'black': 84,\n",
       " 'pride': 85,\n",
       " 'jerks': 86,\n",
       " 'start-ups': 87,\n",
       " 'ruin': 88,\n",
       " 'needs': 89,\n",
       " 'brain': 90,\n",
       " 'manhood': 91,\n",
       " 'age': 92,\n",
       " 'value': 93,\n",
       " 'college': 94,\n",
       " 'initial': 95,\n",
       " 'description': 96,\n",
       " 'rough': 97,\n",
       " 'estimates': 98,\n",
       " 'el': 99,\n",
       " 'pasatiempo': 100,\n",
       " 'nacional': 101,\n",
       " 'cooling': 102,\n",
       " 'off': 103,\n",
       " 'on': 104,\n",
       " 'hot': 105,\n",
       " 'day': 106,\n",
       " 'yankee': 107,\n",
       " 'stadium': 108,\n",
       " 'trump’s': 109,\n",
       " 'staff': 110,\n",
       " 'mixed': 111,\n",
       " 'politics': 112,\n",
       " 'paydays': 113,\n",
       " 'virtuoso': 114,\n",
       " 'rebuilding': 115,\n",
       " 'act': 116,\n",
       " 'requires': 117,\n",
       " 'everyone': 118,\n",
       " 'tune': 119,\n",
       " '‘homeland,’': 120,\n",
       " 'season': 121,\n",
       " '6,': 122,\n",
       " 'episode': 123,\n",
       " '11:': 124,\n",
       " 'quinn': 125,\n",
       " 'just': 126,\n",
       " 'natural': 127,\n",
       " 'killer?': 128,\n",
       " '‘big': 129,\n",
       " 'little': 130,\n",
       " 'lies’': 131,\n",
       " 'art': 132,\n",
       " 'empathy': 133,\n",
       " 'upending': 134,\n",
       " 'whodunit': 135,\n",
       " '‘feud:': 136,\n",
       " 'bette': 137,\n",
       " 'joan’': 138,\n",
       " '5:': 139,\n",
       " 'stage': 140,\n",
       " '‘billions’': 141,\n",
       " '2,': 142,\n",
       " '7:': 143,\n",
       " 'greed': 144,\n",
       " 'good.': 145,\n",
       " 'except': 146,\n",
       " 'when': 147,\n",
       " 'it’s': 148,\n",
       " 'not.': 149,\n",
       " 'unknown': 150,\n",
       " 'what’s': 151,\n",
       " 'going': 152,\n",
       " 'picture?': 153,\n",
       " '|': 154,\n",
       " 'april': 155,\n",
       " '3,': 156,\n",
       " '2017': 157,\n",
       " 'have': 158,\n",
       " 'you': 159,\n",
       " 'ever': 160,\n",
       " 'felt': 161,\n",
       " 'pressured': 162,\n",
       " 'by': 163,\n",
       " 'family': 164,\n",
       " 'or': 165,\n",
       " 'others': 166,\n",
       " 'making': 167,\n",
       " 'important': 168,\n",
       " 'decision': 169,\n",
       " 'about': 170,\n",
       " 'future?': 171,\n",
       " 'cornerstone': 172,\n",
       " 'peace': 173,\n",
       " 'risk': 174,\n",
       " 'wimping': 175,\n",
       " 'out': 176,\n",
       " 'trade': 177,\n",
       " 'dwindling': 178,\n",
       " 'odds': 179,\n",
       " 'coincidence': 180,\n",
       " 'what': 181,\n",
       " 'was': 182,\n",
       " 'lenin': 183,\n",
       " 'thinking?': 184,\n",
       " 'mitch': 185,\n",
       " 'mcconnell’s': 186,\n",
       " 'trigger': 187,\n",
       " 'finger': 188,\n",
       " 'ad': 189,\n",
       " 'north': 190,\n",
       " 'korea': 191,\n",
       " 'yields': 192,\n",
       " 'nuclear': 193,\n",
       " 'clues': 194,\n",
       " 'good': 195,\n",
       " 'news': 196,\n",
       " 'older': 197,\n",
       " 'mothers': 198,\n",
       " 'does': 199,\n",
       " 'birth': 200,\n",
       " 'control': 201,\n",
       " 'cause': 202,\n",
       " 'depression?': 203,\n",
       " 'turning': 204,\n",
       " 'negative': 205,\n",
       " 'thinkers': 206,\n",
       " 'positive': 207,\n",
       " 'ones': 208,\n",
       " 'new': 209,\n",
       " 'york': 210,\n",
       " 'today:': 211,\n",
       " 'belated': 212,\n",
       " 'middlebury,': 213,\n",
       " 'divided': 214,\n",
       " 'campus': 215,\n",
       " 'democrats’': 216,\n",
       " 'vow': 217,\n",
       " 'to': 218,\n",
       " 'bar': 219,\n",
       " 'gorsuch': 220,\n",
       " 'sets': 221,\n",
       " 'clash': 222,\n",
       " 'h-1b': 223,\n",
       " 'visa': 224,\n",
       " 'applications': 225,\n",
       " 'pour': 226,\n",
       " 'truckload': 227,\n",
       " 'amplified': 228,\n",
       " 'world,': 229,\n",
       " 'critics': 230,\n",
       " 'take': 231,\n",
       " 'aim': 232,\n",
       " 'referees': 233,\n",
       " 'revered': 234,\n",
       " 'milwaukee': 235,\n",
       " 'restaurant,': 236,\n",
       " 'karl': 237,\n",
       " 'ratzsch,': 238,\n",
       " 'says': 239,\n",
       " 'goodbye': 240,\n",
       " 'n.h.l.': 241,\n",
       " 'its': 242,\n",
       " 'players': 243,\n",
       " 'will': 244,\n",
       " 'skip': 245,\n",
       " '2018': 246,\n",
       " 'olympics': 247,\n",
       " 'company': 248,\n",
       " 'classic': 249,\n",
       " 'nasty': 250,\n",
       " 'woman': 251,\n",
       " '22': 252,\n",
       " 'ways': 253,\n",
       " 'teach': 254,\n",
       " 'learn': 255,\n",
       " 'poetry': 256,\n",
       " 'with': 257,\n",
       " 'times': 258,\n",
       " 'search': 259,\n",
       " 'king': 260,\n",
       " 'solomon’s': 261,\n",
       " 'pantry': 262,\n",
       " 'back': 263,\n",
       " 'move': 264,\n",
       " 'forward': 265,\n",
       " 'peek': 266,\n",
       " 'white': 267,\n",
       " 'house': 268,\n",
       " 'swamp': 269,\n",
       " 'disney': 270,\n",
       " 'character': 271,\n",
       " 'living': 272,\n",
       " 'quirky': 273,\n",
       " 'dream': 274,\n",
       " 'justice': 275,\n",
       " 'dept.': 276,\n",
       " 're-examine': 277,\n",
       " 'police': 278,\n",
       " 'accords': 279,\n",
       " 'year,': 280,\n",
       " 'their': 281,\n",
       " 'year': 282,\n",
       " 'latest': 283,\n",
       " 'health': 284,\n",
       " 'proposal': 285,\n",
       " 'weakens': 286,\n",
       " 'coverage': 287,\n",
       " 'pre-existing': 288,\n",
       " 'conditions': 289,\n",
       " 'losing': 290,\n",
       " 'let’s': 291,\n",
       " 'go': 292,\n",
       " 'win': 293,\n",
       " 'opioids': 294,\n",
       " 'florida’s': 295,\n",
       " 'vengeful': 296,\n",
       " 'governor': 297,\n",
       " 'how': 298,\n",
       " 'end': 299,\n",
       " 'politicization': 300,\n",
       " 'courts': 301,\n",
       " 'dr.': 302,\n",
       " 'came': 303,\n",
       " 'against': 304,\n",
       " 'vietnam': 305,\n",
       " 'britain’s': 306,\n",
       " 'trains': 307,\n",
       " 'don’t': 308,\n",
       " 'run': 309,\n",
       " 'time.': 310,\n",
       " 'blame': 311,\n",
       " 'capitalism.': 312,\n",
       " 'questions': 313,\n",
       " 'for:': 314,\n",
       " '‘no': 315,\n",
       " 'license': 316,\n",
       " 'plates': 317,\n",
       " 'here:': 318,\n",
       " 'using': 319,\n",
       " 'transcend': 320,\n",
       " 'prison': 321,\n",
       " 'walls’': 322,\n",
       " 'dry': 323,\n",
       " 'spell': 324,\n",
       " 'are': 325,\n",
       " 'there': 326,\n",
       " 'subjects': 327,\n",
       " 'that': 328,\n",
       " 'should': 329,\n",
       " 'be': 330,\n",
       " 'off-limits': 331,\n",
       " 'artists,': 332,\n",
       " 'certain': 333,\n",
       " 'artists': 334,\n",
       " 'particular?': 335,\n",
       " '‘that': 336,\n",
       " 'great': 337,\n",
       " 'television’': 338,\n",
       " 'thinking': 339,\n",
       " 'code': 340,\n",
       " 'gorsuch’s': 341,\n",
       " 'influence': 342,\n",
       " 'could': 343,\n",
       " 'greater': 344,\n",
       " 'than': 345,\n",
       " 'his': 346,\n",
       " 'vote': 347,\n",
       " 'ease': 348,\n",
       " 'hangover': 349,\n",
       " 'gifts': 350,\n",
       " 'china': 351,\n",
       " 'penn': 352,\n",
       " 'station,': 353,\n",
       " 'rail': 354,\n",
       " 'mishap': 355,\n",
       " 'spurs': 356,\n",
       " 'large': 357,\n",
       " 'lasting': 358,\n",
       " 'headache': 359,\n",
       " 'chemical': 360,\n",
       " 'attack': 361,\n",
       " 'syrians': 362,\n",
       " 'ignites': 363,\n",
       " 'world’s': 364,\n",
       " 'outrage': 365,\n",
       " 'adventure': 366,\n",
       " 'still': 367,\n",
       " 'babbo’s': 368,\n",
       " 'menu': 369,\n",
       " 'swimming': 370,\n",
       " 'fast': 371,\n",
       " 'lane': 372,\n",
       " 'national': 373,\n",
       " 'civics': 374,\n",
       " 'exam': 375,\n",
       " 'obama': 376,\n",
       " 'adviser': 377,\n",
       " 'political': 378,\n",
       " 'cross': 379,\n",
       " 'hairs': 380,\n",
       " 'hippies': 381,\n",
       " 'won': 382,\n",
       " 'check': 383,\n",
       " 'box': 384,\n",
       " 'if': 385,\n",
       " 'you’re': 386,\n",
       " 'person': 387,\n",
       " 'couscous,': 388,\n",
       " 'chef’s': 389,\n",
       " 'patience': 390,\n",
       " 'pays': 391,\n",
       " 'three': 392,\n",
       " 'peas': 393,\n",
       " 'grain,': 394,\n",
       " 'playing': 395,\n",
       " 'well': 396,\n",
       " 'together': 397,\n",
       " 'fox': 398,\n",
       " 'like': 399,\n",
       " 'eagles?': 400,\n",
       " 'tired': 401,\n",
       " '‘hamilton’': 402,\n",
       " 'talk': 403,\n",
       " 'supreme': 404,\n",
       " 'court': 405,\n",
       " 'partisan': 406,\n",
       " 'tool': 407,\n",
       " '2': 408,\n",
       " 'picks': 409,\n",
       " 'education': 410,\n",
       " 'raise': 411,\n",
       " 'fears': 412,\n",
       " 'civil': 413,\n",
       " 'rights': 414,\n",
       " 'trump,': 415,\n",
       " 'focus': 416,\n",
       " 'u.s.': 417,\n",
       " 'interests': 418,\n",
       " 'disdain': 419,\n",
       " 'moralizing': 420,\n",
       " 'center': 421,\n",
       " 'universe': 422,\n",
       " '‘the': 423,\n",
       " 'americans’': 424,\n",
       " '5,': 425,\n",
       " '5': 426,\n",
       " 'recap:': 427,\n",
       " 'whole': 428,\n",
       " 'lotta': 429,\n",
       " 'shakin’': 430,\n",
       " 'badger': 431,\n",
       " 'cow': 432,\n",
       " 'jared': 433,\n",
       " 'kushner,': 434,\n",
       " 'man': 435,\n",
       " 'steel': 436,\n",
       " 'how-to': 437,\n",
       " 'book': 438,\n",
       " 'wielding': 439,\n",
       " 'civic': 440,\n",
       " 'power': 441,\n",
       " 'emperor': 442,\n",
       " 'real-world': 443,\n",
       " 'syria': 444,\n",
       " 'lesson': 445,\n",
       " '‘spacex': 446,\n",
       " 'launches': 447,\n",
       " 'satellite': 448,\n",
       " 'partly': 449,\n",
       " 'used': 450,\n",
       " 'rocket’': 451,\n",
       " 'ben': 452,\n",
       " 'sasse': 453,\n",
       " 'thinks': 454,\n",
       " 'biden': 455,\n",
       " 'would’ve': 456,\n",
       " 'earliest': 457,\n",
       " 'memory?': 458,\n",
       " 'secularist': 459,\n",
       " 'it': 460,\n",
       " 'o.k.': 461,\n",
       " 'our': 462,\n",
       " 'friends': 463,\n",
       " 'constantly': 464,\n",
       " 'suing': 465,\n",
       " 'people?': 466,\n",
       " 'diva': 467,\n",
       " 'departs': 468,\n",
       " 'takes': 469,\n",
       " 'suburb': 470,\n",
       " 'democratic': 471,\n",
       " 'turnout,': 472,\n",
       " 'low': 473,\n",
       " 'off-year': 474,\n",
       " 'races,': 475,\n",
       " 'appears': 476,\n",
       " 'rise': 477,\n",
       " 'mindful': 478,\n",
       " 'angry': 479,\n",
       " 'lessons': 480,\n",
       " 'mellow': 481,\n",
       " 'mice': 482,\n",
       " 'ask': 483,\n",
       " 'well;': 484,\n",
       " 'chewing': 485,\n",
       " 'gum': 486,\n",
       " 'toddlers?': 487,\n",
       " 'anyone?': 488,\n",
       " 'another': 489,\n",
       " 'thorny': 490,\n",
       " 'commute': 491,\n",
       " 'eighth': 492,\n",
       " 'annual': 493,\n",
       " 'found': 494,\n",
       " 'poem': 495,\n",
       " 'student': 496,\n",
       " 'contest': 497,\n",
       " 'women’s': 498,\n",
       " 'soccer': 499,\n",
       " 'team': 500,\n",
       " 'wins': 501,\n",
       " 'more,': 502,\n",
       " 'not': 503,\n",
       " 'equal,': 504,\n",
       " 'pay': 505,\n",
       " 'really': 506,\n",
       " 'security': 507,\n",
       " 'rationale': 508,\n",
       " 'banning': 509,\n",
       " 'laptops': 510,\n",
       " 'planes?': 511,\n",
       " 'seeking': 512,\n",
       " 'truths': 513,\n",
       " 'locked': 514,\n",
       " 'inside': 515,\n",
       " 'child': 516,\n",
       " 'soldier': 517,\n",
       " 'pepsi': 518,\n",
       " 'drops': 519,\n",
       " 'accused': 520,\n",
       " 'trivializing': 521,\n",
       " 'protesters': 522,\n",
       " 'eleven': 523,\n",
       " 'madison': 524,\n",
       " 'park': 525,\n",
       " 'tops': 526,\n",
       " 'list': 527,\n",
       " '50': 528,\n",
       " 'best': 529,\n",
       " 'restaurants': 530,\n",
       " 'bannon': 531,\n",
       " 'removed': 532,\n",
       " 'committee': 533,\n",
       " 'hawk': 534,\n",
       " 'soar': 535,\n",
       " 'suggests': 536,\n",
       " 'bigger': 537,\n",
       " 'role': 538,\n",
       " 'conflict': 539,\n",
       " 'cast': 540,\n",
       " 'doubt': 541,\n",
       " 'infrastructure': 542,\n",
       " 'groups': 543,\n",
       " 'seek': 544,\n",
       " 'court’s': 545,\n",
       " 'aid': 546,\n",
       " 'pesticide': 547,\n",
       " 'lobbying': 548,\n",
       " 'fierce': 549,\n",
       " '(and': 550,\n",
       " 'tasty)': 551,\n",
       " 'passing': 552,\n",
       " 'moe': 553,\n",
       " 'mr.': 554,\n",
       " 'most': 555,\n",
       " 'meeting': 556,\n",
       " 'dies,': 557,\n",
       " 'republicans': 558,\n",
       " 'can’t': 559,\n",
       " 'agree': 560,\n",
       " 'culprit': 561,\n",
       " 'fine': 562,\n",
       " 'romance?': 563,\n",
       " 'yes,': 564,\n",
       " 'gay': 565,\n",
       " 'history': 566,\n",
       " 'tour': 567,\n",
       " 'playbook': 568,\n",
       " 'symbols': 569,\n",
       " 'serving': 570,\n",
       " 'ham': 571,\n",
       " 'soignée': 572,\n",
       " 'silk': 573,\n",
       " 'fashion': 574,\n",
       " 'has': 575,\n",
       " 'covered': 576,\n",
       " 'only': 577,\n",
       " 'small': 578,\n",
       " 'army': 579,\n",
       " 'equal': 580,\n",
       " 'arnie': 581,\n",
       " 'public': 582,\n",
       " 'broadcasting': 583,\n",
       " 'learned': 584,\n",
       " 'younger': 585,\n",
       " '—': 586,\n",
       " 'taught': 587,\n",
       " 'person?': 588,\n",
       " 'i': 589,\n",
       " 'angered': 590,\n",
       " 'readers,': 591,\n",
       " 'again': 592,\n",
       " 'creeping': 593,\n",
       " 'toward': 594,\n",
       " 'crisis': 595,\n",
       " 'mistake': 596,\n",
       " 'war': 597,\n",
       " 'bets': 598,\n",
       " 'messy': 599,\n",
       " 'state': 600,\n",
       " 'u.s.-china': 601,\n",
       " 'ties:': 602,\n",
       " 'do': 603,\n",
       " '‘haley': 604,\n",
       " 'may': 605,\n",
       " '‘take': 606,\n",
       " 'own': 607,\n",
       " 'action’': 608,\n",
       " 'syrian': 609,\n",
       " 'attack’': 610,\n",
       " 'shadow': 611,\n",
       " 'fairy': 612,\n",
       " 'tale': 613,\n",
       " 'gut': 614,\n",
       " 'filibuster': 615,\n",
       " 'rule': 616,\n",
       " 'lift': 617,\n",
       " 'berliner': 618,\n",
       " 'fernsehturm': 619,\n",
       " 'improving': 620,\n",
       " 'original': 621,\n",
       " 'reader': 622,\n",
       " 'stories': 623,\n",
       " 'independence': 624,\n",
       " 'days': 625,\n",
       " 'snooping': 626,\n",
       " 'teenagers': 627,\n",
       " 'o.k.?': 628,\n",
       " 'city': 629,\n",
       " 'train': 630,\n",
       " 'donkey,': 631,\n",
       " 'find': 632,\n",
       " 'zebra': 633,\n",
       " 'facing': 634,\n",
       " 'scrutiny,': 635,\n",
       " 'intelligence': 636,\n",
       " 'chief': 637,\n",
       " 'leaves': 638,\n",
       " 'russia': 639,\n",
       " 'inquiry': 640,\n",
       " 'heart': 641,\n",
       " 'amazon': 642,\n",
       " '‘mrs.’': 643,\n",
       " 'became': 644,\n",
       " '‘ms.’': 645,\n",
       " 'recruiting': 646,\n",
       " 'assistants': 647,\n",
       " 'top': 648,\n",
       " 'job': 649,\n",
       " 'ewing': 650,\n",
       " 'dieting:': 651,\n",
       " 'yo-yo': 652,\n",
       " 'diets': 653,\n",
       " 'heart:': 654,\n",
       " 'race': 655,\n",
       " 'factor': 656,\n",
       " 'mixing': 657,\n",
       " 'drinks': 658,\n",
       " 'message': 659,\n",
       " 'simple': 660,\n",
       " 'taiwanese': 661,\n",
       " 'food': 662,\n",
       " 'doting': 663,\n",
       " 'mama': 664,\n",
       " 'don': 665,\n",
       " 'rickles,': 666,\n",
       " 'comedy’s': 667,\n",
       " 'opportunity': 668,\n",
       " 'offender,': 669,\n",
       " 'dies': 670,\n",
       " '90': 671,\n",
       " 'beaujolais,': 672,\n",
       " 'nouveau': 673,\n",
       " 'missiles': 674,\n",
       " 'feel': 675,\n",
       " 'american,': 676,\n",
       " 'try': 677,\n",
       " 'some': 678,\n",
       " 'velveeta': 679,\n",
       " 'fighting': 680,\n",
       " 'eviction,': 681,\n",
       " 'gardener': 682,\n",
       " 'turns': 683,\n",
       " 'organic': 684,\n",
       " 'industry': 685,\n",
       " 'giants': 686,\n",
       " 'help': 687,\n",
       " 'vaccines:': 688,\n",
       " 'moms’': 689,\n",
       " 'shot': 690,\n",
       " 'protects': 691,\n",
       " 'newborns': 692,\n",
       " 'without': 693,\n",
       " 'rikers': 694,\n",
       " 'island,': 695,\n",
       " 'learning': 696,\n",
       " 'love': 697,\n",
       " 'jail': 698,\n",
       " 'next': 699,\n",
       " 'door': 700,\n",
       " 'fingers': 701,\n",
       " 'crossed': 702,\n",
       " 'across': 703,\n",
       " 'generations': 704,\n",
       " 'donald': 705,\n",
       " 'bill': 706,\n",
       " 'roger': 707,\n",
       " 'ailes': 708,\n",
       " 'common?': 709,\n",
       " 'downsizing': 710,\n",
       " 'rock': 711,\n",
       " 'music': 712,\n",
       " 'sing': 713,\n",
       " 'song': 714,\n",
       " 'face': 715,\n",
       " 'creams': 716,\n",
       " 'after': 717,\n",
       " 'missiles,': 718,\n",
       " 'we': 719,\n",
       " 'need': 720,\n",
       " 'smart': 721,\n",
       " 'diplomacy': 722,\n",
       " 'youth,': 723,\n",
       " 'crowds,': 724,\n",
       " 'goals:': 725,\n",
       " 'germany': 726,\n",
       " 'aims': 727,\n",
       " 'keep': 728,\n",
       " 'way': 729,\n",
       " 'soul': 730,\n",
       " '…': 731,\n",
       " 'corporation': 732,\n",
       " 'coming': 733,\n",
       " 'incompetence': 734,\n",
       " 'bad,': 735,\n",
       " 'worse': 736,\n",
       " 'ugly': 737,\n",
       " 'optimists': 738,\n",
       " 'pessimists': 739,\n",
       " 'more': 740,\n",
       " 'journalists': 741,\n",
       " 'independent': 742,\n",
       " 'editorial': 743,\n",
       " 'control?': 744,\n",
       " 'ghost': 745,\n",
       " 'airbnb': 746,\n",
       " 'borrowers': 747,\n",
       " 'bewildered': 748,\n",
       " 'unemployment': 749,\n",
       " 'falls,': 750,\n",
       " 'feeble': 751,\n",
       " 'growth': 752,\n",
       " 'tempers': 753,\n",
       " 'optimism': 754,\n",
       " 'friday': 755,\n",
       " 'mailbag:': 756,\n",
       " 'senate': 757,\n",
       " 'votes': 758,\n",
       " 'museum': 759,\n",
       " 'drama': 760,\n",
       " 'teaching': 761,\n",
       " 'with:': 762,\n",
       " '‘animated': 763,\n",
       " 'life:': 764,\n",
       " 'mary': 765,\n",
       " 'leakey’': 766,\n",
       " 'muppet': 767,\n",
       " 'autism': 768,\n",
       " 'means': 769,\n",
       " 'preschool': 770,\n",
       " 'teachers': 771,\n",
       " 'graduates?': 772,\n",
       " 'no': 773,\n",
       " 'longer': 774,\n",
       " 'citi': 775,\n",
       " 'field’s': 776,\n",
       " 'alpha': 777,\n",
       " 'dog,': 778,\n",
       " 'harvey': 779,\n",
       " 'bite': 780,\n",
       " 'mexico': 781,\n",
       " 'outlaws': 782,\n",
       " 'school': 783,\n",
       " '‘lunch': 784,\n",
       " 'shaming’': 785,\n",
       " 'economic': 786,\n",
       " 'indicator?': 787,\n",
       " 'isn’t': 788,\n",
       " 'riddle': 789,\n",
       " 'strike': 790,\n",
       " 'air': 791,\n",
       " 'base': 792,\n",
       " 'angers': 793,\n",
       " 'russians': 794,\n",
       " 'nominee': 795,\n",
       " 'confirmed': 796,\n",
       " 'bruising': 797,\n",
       " 'yearlong': 798,\n",
       " 'fight': 799,\n",
       " 'disquieting': 800,\n",
       " 'silence': 801,\n",
       " 'near-zero': 802,\n",
       " 'interest': 803,\n",
       " 'rates:': 804,\n",
       " 'get': 805,\n",
       " 'them': 806,\n",
       " 'slice': 807,\n",
       " 'ambrosia': 808,\n",
       " 'filling)': 809,\n",
       " 'survived': 810,\n",
       " 'sarin': 811,\n",
       " 'gas': 812,\n",
       " 'first': 813,\n",
       " 'televised': 814,\n",
       " 'airstrikes,': 815,\n",
       " 'next?': 816,\n",
       " 'romance,': 817,\n",
       " 'sarcasm,': 818,\n",
       " 'math': 819,\n",
       " 'language': 820,\n",
       " 'who': 821,\n",
       " 'put': 822,\n",
       " 'those': 823,\n",
       " 'endless': 824,\n",
       " 'tunes': 825,\n",
       " 'fumes?': 826,\n",
       " 'share': 827,\n",
       " 'pta': 828,\n",
       " 'aid?': 829,\n",
       " 'parents': 830,\n",
       " 'would': 831,\n",
       " 'rather': 832,\n",
       " 'split': 833,\n",
       " 'district': 834,\n",
       " 'election': 835,\n",
       " 'left': 836,\n",
       " 'over': 837,\n",
       " 'economy': 838,\n",
       " 'marching': 839,\n",
       " 'bands': 840,\n",
       " 'fuels': 841,\n",
       " 'uncertainty': 842,\n",
       " 'ground': 843,\n",
       " 'clinton,': 844,\n",
       " 'free': 845,\n",
       " 'speak': 846,\n",
       " 'her': 847,\n",
       " 'mind': 848,\n",
       " 'passion': 849,\n",
       " 'southern': 850,\n",
       " 'christians': 851,\n",
       " 'wall': 852,\n",
       " 'happened': 853,\n",
       " 'who?': 854,\n",
       " 'president’s': 855,\n",
       " 'generals': 856,\n",
       " 'myth': 857,\n",
       " 'main': 858,\n",
       " 'street': 859,\n",
       " 'interviews': 860,\n",
       " 'offbeat': 861,\n",
       " 'approach': 862,\n",
       " 'covering': 863,\n",
       " 'sports': 864,\n",
       " 'having': 865,\n",
       " 'nothing': 866,\n",
       " 'getting': 867,\n",
       " 'canary': 868,\n",
       " 'tillerson': 869,\n",
       " 'halts': 870,\n",
       " 'any': 871,\n",
       " 'thawing': 872,\n",
       " 'ties': 873,\n",
       " 'words': 874,\n",
       " 'self-empowerment': 875,\n",
       " 'claiming': 876,\n",
       " 'odyssey’s': 877,\n",
       " 'reward': 878,\n",
       " 'familiar': 879,\n",
       " 'signature': 880,\n",
       " 'suits': 881,\n",
       " 'say': 882,\n",
       " 'lender': 883,\n",
       " 'duped': 884,\n",
       " 'students': 885,\n",
       " 'fuel': 886,\n",
       " 'station': 887,\n",
       " 'tie-ups': 888,\n",
       " 'depend': 889,\n",
       " 'knot': 890,\n",
       " 'agencies': 891,\n",
       " 'same': 892,\n",
       " 'old': 893,\n",
       " 'sergio': 894,\n",
       " 'delights': 895,\n",
       " 'crowd': 896,\n",
       " 'script': 897,\n",
       " '8:': 898,\n",
       " 'money,': 899,\n",
       " 'rules': 900,\n",
       " '6': 901,\n",
       " 'midnight': 902,\n",
       " 'descending': 903,\n",
       " 'spring': 904,\n",
       " 'break': 905,\n",
       " 'network': 906,\n",
       " '12:': 907,\n",
       " 'finale,': 908,\n",
       " 'carrie': 909,\n",
       " 'deals': 910,\n",
       " 'death': 911,\n",
       " 'betrayal': 912,\n",
       " 'bo': 913,\n",
       " 'diddley': 914,\n",
       " 'buddha?': 915,\n",
       " 'gig': 916,\n",
       " 'economy’s': 917,\n",
       " 'false': 918,\n",
       " 'promise': 919,\n",
       " 'publicity': 920,\n",
       " 'stunts': 921,\n",
       " 'aren’t': 922,\n",
       " 'policy': 923,\n",
       " 'road': 924,\n",
       " 'weapon': 925,\n",
       " 'alabama': 926,\n",
       " 'resigns': 927,\n",
       " 'pleads': 928,\n",
       " 'guilty': 929,\n",
       " 'amid': 930,\n",
       " 'sex': 931,\n",
       " 'scandal': 932,\n",
       " 'california': 933,\n",
       " 'moves': 934,\n",
       " 'protections': 935,\n",
       " 'immigrants;': 936,\n",
       " 'states': 937,\n",
       " 'follow': 938,\n",
       " 'he': 939,\n",
       " 'led': 940,\n",
       " 'yankees': 941,\n",
       " '4': 942,\n",
       " 'titles.': 943,\n",
       " 'revive': 944,\n",
       " 'them?': 945,\n",
       " 'fox:': 946,\n",
       " 'women': 947,\n",
       " 'often': 948,\n",
       " 'report': 949,\n",
       " 'sexual': 950,\n",
       " 'harassment': 951,\n",
       " 'work': 952,\n",
       " 'parents’': 953,\n",
       " 'mistakes': 954,\n",
       " 'rude': 955,\n",
       " 'doctors,': 956,\n",
       " 'nurses,': 957,\n",
       " 'patients': 958,\n",
       " 'passover,': 959,\n",
       " 'everyday': 960,\n",
       " 'plagues': 961,\n",
       " 'many': 962,\n",
       " 'pills': 963,\n",
       " 'too': 964,\n",
       " 'many?': 965,\n",
       " 'greatest': 966,\n",
       " 'earth': 967,\n",
       " 'wells': 968,\n",
       " 'fargo': 969,\n",
       " 'ex-leaders': 970,\n",
       " 'owe': 971,\n",
       " '$75': 972,\n",
       " 'million': 973,\n",
       " 'saved': 974,\n",
       " 'patients,': 975,\n",
       " 'furious': 976,\n",
       " 'families': 977,\n",
       " 'russian': 978,\n",
       " '‘boris': 979,\n",
       " 'badenov’': 980,\n",
       " 'high': 981,\n",
       " 'highlight': 982,\n",
       " 'president,': 983,\n",
       " 'sworn': 984,\n",
       " '113th': 985,\n",
       " 'deployment': 986,\n",
       " 'carrier': 987,\n",
       " 'masks': 988,\n",
       " 'lack': 989,\n",
       " 'better': 990,\n",
       " 'options': 991,\n",
       " 'dragged': 992,\n",
       " 'full': 993,\n",
       " 'jet,': 994,\n",
       " 'stirring': 995,\n",
       " 'furor': 996,\n",
       " '3': 997,\n",
       " 'pulitzers;': 998,\n",
       " 'service': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation]\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112f014",
   "metadata": {},
   "source": [
    "##### 만든 문장을 토크나이저 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8a4ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 49, 50], 'variety puzzle: acrostic')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시\n",
    "[bow[w] for w in cleaned_sentence[10].split()], cleaned_sentence[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87793718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle에서 데이터 download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv 파일이 있는 경로 path\n",
    "# csv의 절대경로 리스트\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values)) # ^ [[]] 형태가 안되도록, append\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "\n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그 다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "            for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcda4862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array(2., dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextGeneration(csv_lists)\n",
    "len(dataset)\n",
    "x, y  = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b5eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터가 2개의 array로 만들어지는 과정 정리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643595c8",
   "metadata": {},
   "source": [
    "##### LSTM 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9ff650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,num_embeddings): # num_embeddings 전체 단어의 개수(어휘사전 크기)\n",
    "        super(LSTM, self).__init__()\n",
    "        # 신경망이 이해할 수 있도록 벡터로 변경\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
    "        \n",
    "        # LSTM을 5개 층 (배치, 시퀀스, 피처) 16 ~ 512\n",
    "        self.lstm = nn.LSTM(input_size=16,hidden_size=64,num_layers=5,batch_first=True)\n",
    "        # 분류를 위한 fc 층\n",
    "        self.fc1 = nn.Linear( 128 , num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings ,num_embeddings)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x): # 입력 (배치크기 , sequence_length) batch : 32, sq_len : 2\n",
    "                         # 출력 (배치크기, sequence_length, 16) 32 2 16\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # lstm 모델 예측값\n",
    "        x, _ = self.lstm(x) # 츨력 (batch, sq_len, 64) 32 2 64\n",
    "        x = torch.reshape(x,(x.shape[0],-1)) # 출력 (batch, sq_len x 64) 32, 128\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ad555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\Playdata2\\AppData\\Local\\Temp\\ipykernel_17260\\3789114445.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long))\n",
      "C:\\Users\\Playdata2\\AppData\\Local\\Temp\\ipykernel_17260\\3789114445.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype=torch.long))\n",
      "epoch : 1loss : 7.375741481781006: 100%|██████████| 63/63 [00:04<00:00, 14.84it/s] \n",
      "epoch : 2loss : 7.022435665130615: 100%|██████████| 63/63 [00:04<00:00, 14.64it/s] \n",
      "epoch : 3loss : 6.773029327392578: 100%|██████████| 63/63 [00:04<00:00, 14.41it/s] \n",
      "epoch : 4loss : 6.5218658447265625: 100%|██████████| 63/63 [00:04<00:00, 13.38it/s]\n",
      "epoch : 5loss : 6.431317329406738: 100%|██████████| 63/63 [00:04<00:00, 14.14it/s] \n",
      "epoch : 6loss : 6.328286170959473: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s] \n",
      "epoch : 7loss : 6.162210464477539: 100%|██████████| 63/63 [00:04<00:00, 15.43it/s] \n",
      "epoch : 8loss : 5.968717098236084: 100%|██████████| 63/63 [00:04<00:00, 15.14it/s] \n",
      "epoch : 9loss : 5.797878742218018: 100%|██████████| 63/63 [00:04<00:00, 14.55it/s] \n",
      "epoch : 10loss : 5.588218688964844: 100%|██████████| 63/63 [00:04<00:00, 14.57it/s] \n",
      "epoch : 11loss : 5.488147735595703: 100%|██████████| 63/63 [00:04<00:00, 14.45it/s] \n",
      "epoch : 12loss : 5.443409442901611: 100%|██████████| 63/63 [00:04<00:00, 13.44it/s] \n",
      "epoch : 13loss : 5.370067596435547: 100%|██████████| 63/63 [00:04<00:00, 14.38it/s] \n",
      "epoch : 14loss : 5.335476398468018: 100%|██████████| 63/63 [00:04<00:00, 13.98it/s] \n",
      "epoch : 15loss : 5.247236251831055: 100%|██████████| 63/63 [00:04<00:00, 13.08it/s] \n",
      "epoch : 16loss : 5.274612903594971: 100%|██████████| 63/63 [00:05<00:00, 12.40it/s] \n",
      "epoch : 17loss : 5.142375469207764: 100%|██████████| 63/63 [00:05<00:00, 12.45it/s] \n",
      "epoch : 18loss : 4.962790489196777: 100%|██████████| 63/63 [00:05<00:00, 12.40it/s] \n",
      "epoch : 19loss : 5.026055335998535: 100%|██████████| 63/63 [00:05<00:00, 11.90it/s] \n",
      "epoch : 20loss : 4.739928722381592: 100%|██████████| 63/63 [00:05<00:00, 12.35it/s] \n",
      "epoch : 21loss : 4.857210159301758: 100%|██████████| 63/63 [00:05<00:00, 12.37it/s] \n",
      "epoch : 22loss : 4.8478899002075195: 100%|██████████| 63/63 [00:05<00:00, 12.40it/s]\n",
      "epoch : 23loss : 4.812654972076416: 100%|██████████| 63/63 [00:05<00:00, 12.09it/s] \n",
      "epoch : 24loss : 4.661837100982666: 100%|██████████| 63/63 [00:05<00:00, 11.79it/s] \n",
      "epoch : 25loss : 4.571989059448242: 100%|██████████| 63/63 [00:05<00:00, 11.58it/s] \n",
      "epoch : 26loss : 4.443070411682129: 100%|██████████| 63/63 [00:06<00:00, 10.40it/s] \n",
      "epoch : 27loss : 4.333888530731201: 100%|██████████| 63/63 [00:05<00:00, 10.90it/s] \n",
      "epoch : 28loss : 4.26584005355835: 100%|██████████| 63/63 [00:05<00:00, 11.93it/s]  \n",
      "epoch : 29loss : 4.151033401489258: 100%|██████████| 63/63 [00:05<00:00, 11.90it/s] \n",
      "epoch : 30loss : 4.094428062438965: 100%|██████████| 63/63 [00:05<00:00, 11.35it/s] \n",
      "epoch : 31loss : 4.244422435760498: 100%|██████████| 63/63 [00:05<00:00, 11.61it/s] \n",
      "epoch : 32loss : 4.547124862670898: 100%|██████████| 63/63 [00:05<00:00, 11.80it/s] \n",
      "epoch : 33loss : 4.208241939544678: 100%|██████████| 63/63 [00:05<00:00, 11.40it/s] \n",
      "epoch : 34loss : 3.9087681770324707: 100%|██████████| 63/63 [00:05<00:00, 11.45it/s]\n",
      "epoch : 35loss : 3.815399646759033: 100%|██████████| 63/63 [00:05<00:00, 11.76it/s] \n",
      "epoch : 36loss : 3.7867507934570312: 100%|██████████| 63/63 [00:05<00:00, 11.77it/s]\n",
      "epoch : 37loss : 3.7369766235351562: 100%|██████████| 63/63 [00:05<00:00, 11.30it/s]\n",
      "epoch : 38loss : 3.676769971847534: 100%|██████████| 63/63 [00:05<00:00, 11.31it/s] \n",
      "epoch : 39loss : 3.650165557861328: 100%|██████████| 63/63 [00:05<00:00, 12.11it/s] \n",
      "epoch : 40loss : 3.692767381668091: 100%|██████████| 63/63 [00:05<00:00, 11.78it/s] \n",
      "epoch : 41loss : 3.7926158905029297: 100%|██████████| 63/63 [00:05<00:00, 11.09it/s]\n",
      "epoch : 42loss : 4.008126735687256: 100%|██████████| 63/63 [00:05<00:00, 11.23it/s] \n",
      "epoch : 43loss : 4.104813575744629: 100%|██████████| 63/63 [00:05<00:00, 11.46it/s] \n",
      "epoch : 44loss : 4.266307830810547: 100%|██████████| 63/63 [00:04<00:00, 14.06it/s] \n",
      "epoch : 45loss : 3.7782135009765625: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]\n",
      "epoch : 46loss : 3.5316295623779297: 100%|██████████| 63/63 [00:04<00:00, 14.03it/s]\n",
      "epoch : 47loss : 3.4371604919433594: 100%|██████████| 63/63 [00:05<00:00, 12.48it/s]\n",
      "epoch : 48loss : 3.4749526977539062: 100%|██████████| 63/63 [00:04<00:00, 13.08it/s]\n",
      "epoch : 49loss : 3.335294008255005: 100%|██████████| 63/63 [00:04<00:00, 13.91it/s] \n",
      "epoch : 50loss : 3.1966958045959473: 100%|██████████| 63/63 [00:04<00:00, 13.32it/s]\n",
      "epoch : 51loss : 3.109610080718994: 100%|██████████| 63/63 [00:04<00:00, 14.08it/s] \n",
      "epoch : 52loss : 3.0898122787475586: 100%|██████████| 63/63 [00:04<00:00, 14.28it/s]\n",
      "epoch : 53loss : 3.036994218826294: 100%|██████████| 63/63 [00:04<00:00, 14.28it/s] \n",
      "epoch : 54loss : 3.0430946350097656: 100%|██████████| 63/63 [00:04<00:00, 14.33it/s]\n",
      "epoch : 55loss : 2.973952054977417: 100%|██████████| 63/63 [00:04<00:00, 14.42it/s] \n",
      "epoch : 56loss : 2.908444404602051: 100%|██████████| 63/63 [00:04<00:00, 14.26it/s] \n",
      "epoch : 57loss : 2.8481321334838867: 100%|██████████| 63/63 [00:04<00:00, 14.38it/s]\n",
      "epoch : 58loss : 2.8711295127868652: 100%|██████████| 63/63 [00:04<00:00, 14.30it/s]\n",
      "epoch : 59loss : 2.9612174034118652: 100%|██████████| 63/63 [00:04<00:00, 14.39it/s]\n",
      "epoch : 60loss : 2.9839909076690674: 100%|██████████| 63/63 [00:04<00:00, 14.17it/s]\n",
      "epoch : 61loss : 2.827822685241699: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s] \n",
      "epoch : 62loss : 3.3538641929626465: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\n",
      "epoch : 63loss : 3.2377777099609375: 100%|██████████| 63/63 [00:04<00:00, 14.13it/s]\n",
      "epoch : 64loss : 3.1602425575256348: 100%|██████████| 63/63 [00:04<00:00, 13.96it/s]\n",
      "epoch : 65loss : 2.9713997840881348: 100%|██████████| 63/63 [00:04<00:00, 14.07it/s]\n",
      "epoch : 66loss : 3.023439884185791: 100%|██████████| 63/63 [00:04<00:00, 14.09it/s] \n",
      "epoch : 67loss : 2.7358274459838867: 100%|██████████| 63/63 [00:04<00:00, 14.11it/s]\n",
      "epoch : 68loss : 2.582327365875244: 100%|██████████| 63/63 [00:04<00:00, 13.77it/s] \n",
      "epoch : 69loss : 2.6028144359588623: 100%|██████████| 63/63 [00:04<00:00, 14.24it/s]\n",
      "epoch : 70loss : 2.782425880432129: 100%|██████████| 63/63 [00:04<00:00, 14.34it/s] \n",
      "epoch : 71loss : 2.984557867050171: 100%|██████████| 63/63 [00:04<00:00, 14.29it/s] \n",
      "epoch : 72loss : 2.9230692386627197: 100%|██████████| 63/63 [00:04<00:00, 14.47it/s]\n",
      "epoch : 73loss : 2.773942708969116: 100%|██████████| 63/63 [00:04<00:00, 14.43it/s] \n",
      "epoch : 74loss : 2.711042881011963: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s] \n",
      "epoch : 75loss : 2.7179388999938965: 100%|██████████| 63/63 [00:04<00:00, 14.36it/s]\n",
      "epoch : 76loss : 2.723118782043457: 100%|██████████| 63/63 [00:04<00:00, 14.20it/s] \n",
      "epoch : 77loss : 2.6666641235351562: 100%|██████████| 63/63 [00:04<00:00, 13.51it/s]\n",
      "epoch : 78loss : 2.6445486545562744: 100%|██████████| 63/63 [00:04<00:00, 12.66it/s]\n",
      "epoch : 79loss : 2.5072407722473145: 100%|██████████| 63/63 [00:04<00:00, 13.79it/s]\n",
      "epoch : 80loss : 2.5032267570495605: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s]\n",
      "epoch : 81loss : 2.4778566360473633: 100%|██████████| 63/63 [00:04<00:00, 13.85it/s]\n",
      "epoch : 82loss : 2.5264382362365723: 100%|██████████| 63/63 [00:04<00:00, 14.12it/s]\n",
      "epoch : 83loss : 2.453373432159424: 100%|██████████| 63/63 [00:04<00:00, 14.42it/s] \n",
      "epoch : 84loss : 2.399418830871582: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s] \n",
      "epoch : 85loss : 2.8910861015319824: 100%|██████████| 63/63 [00:04<00:00, 14.41it/s]\n",
      "epoch : 86loss : 2.9765374660491943: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s]\n",
      "epoch : 87loss : 2.531681776046753: 100%|██████████| 63/63 [00:04<00:00, 14.11it/s] \n",
      "epoch : 88loss : 2.4314770698547363: 100%|██████████| 63/63 [00:04<00:00, 14.43it/s]\n",
      "epoch : 89loss : 2.430203676223755: 100%|██████████| 63/63 [00:04<00:00, 14.04it/s] \n",
      "epoch : 90loss : 2.487527847290039: 100%|██████████| 63/63 [00:04<00:00, 13.79it/s] \n",
      "epoch : 91loss : 2.274369478225708: 100%|██████████| 63/63 [00:04<00:00, 13.77it/s] \n",
      "epoch : 92loss : 2.1587367057800293: 100%|██████████| 63/63 [00:04<00:00, 14.12it/s]\n",
      "epoch : 93loss : 2.234480381011963: 100%|██████████| 63/63 [00:04<00:00, 14.34it/s] \n",
      "epoch : 94loss : 2.3022243976593018: 100%|██████████| 63/63 [00:04<00:00, 14.29it/s]\n",
      "epoch : 95loss : 2.141268014907837: 100%|██████████| 63/63 [00:04<00:00, 13.87it/s] \n",
      "epoch : 96loss : 2.3966872692108154: 100%|██████████| 63/63 [00:04<00:00, 13.68it/s]\n",
      "epoch : 97loss : 2.5940091609954834: 100%|██████████| 63/63 [00:04<00:00, 14.46it/s]\n",
      "epoch : 98loss : 2.5234580039978027: 100%|██████████| 63/63 [00:04<00:00, 13.65it/s]\n",
      "epoch : 99loss : 2.5050196647644043: 100%|██████████| 63/63 [00:05<00:00, 12.00it/s]\n",
      "epoch : 100loss : 2.2564356327056885: 100%|██████████| 63/63 [00:05<00:00, 11.18it/s]\n",
      "epoch : 101loss : 2.124662160873413: 100%|██████████| 63/63 [00:05<00:00, 10.95it/s] \n",
      "epoch : 102loss : 1.961462378501892: 100%|██████████| 63/63 [00:06<00:00, 10.41it/s] \n",
      "epoch : 103loss : 2.018585205078125: 100%|██████████| 63/63 [00:05<00:00, 11.63it/s] \n",
      "epoch : 104loss : 2.057577610015869: 100%|██████████| 63/63 [00:05<00:00, 11.90it/s] \n",
      "epoch : 105loss : 2.0525054931640625: 100%|██████████| 63/63 [00:04<00:00, 13.06it/s]\n",
      "epoch : 106loss : 2.0205981731414795: 100%|██████████| 63/63 [00:04<00:00, 14.44it/s]\n",
      "epoch : 107loss : 1.9447052478790283: 100%|██████████| 63/63 [00:04<00:00, 14.17it/s]\n",
      "epoch : 108loss : 2.087829113006592: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s] \n",
      "epoch : 109loss : 1.9839658737182617: 100%|██████████| 63/63 [00:04<00:00, 14.63it/s]\n",
      "epoch : 110loss : 1.7717205286026: 100%|██████████| 63/63 [00:04<00:00, 14.38it/s]   \n",
      "epoch : 111loss : 2.0674080848693848: 100%|██████████| 63/63 [00:04<00:00, 14.26it/s]\n",
      "epoch : 112loss : 1.9498924016952515: 100%|██████████| 63/63 [00:04<00:00, 14.27it/s]\n",
      "epoch : 113loss : 1.739776611328125: 100%|██████████| 63/63 [00:04<00:00, 14.30it/s] \n",
      "epoch : 114loss : 1.5172107219696045: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s]\n",
      "epoch : 115loss : 1.7895221710205078: 100%|██████████| 63/63 [00:04<00:00, 12.69it/s]\n",
      "epoch : 116loss : 1.7180334329605103: 100%|██████████| 63/63 [00:04<00:00, 13.94it/s]\n",
      "epoch : 117loss : 1.6777311563491821: 100%|██████████| 63/63 [00:04<00:00, 13.95it/s]\n",
      "epoch : 118loss : 1.7845436334609985: 100%|██████████| 63/63 [00:04<00:00, 12.64it/s]\n",
      "epoch : 119loss : 1.67727792263031: 100%|██████████| 63/63 [00:04<00:00, 14.62it/s]  \n",
      "epoch : 120loss : 1.8099130392074585: 100%|██████████| 63/63 [00:04<00:00, 14.33it/s]\n",
      "epoch : 121loss : 1.547548532485962: 100%|██████████| 63/63 [00:04<00:00, 14.76it/s] \n",
      "epoch : 122loss : 1.46916925907135: 100%|██████████| 63/63 [00:04<00:00, 14.09it/s]  \n",
      "epoch : 123loss : 1.432832956314087: 100%|██████████| 63/63 [00:04<00:00, 14.80it/s] \n",
      "epoch : 124loss : 1.5770576000213623: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]\n",
      "epoch : 125loss : 1.5462872982025146: 100%|██████████| 63/63 [00:04<00:00, 14.89it/s]\n",
      "epoch : 126loss : 1.499365210533142: 100%|██████████| 63/63 [00:04<00:00, 14.98it/s] \n",
      "epoch : 127loss : 1.4346227645874023: 100%|██████████| 63/63 [00:04<00:00, 14.73it/s]\n",
      "epoch : 128loss : 1.3391788005828857: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\n",
      "epoch : 129loss : 1.2274290323257446: 100%|██████████| 63/63 [00:04<00:00, 14.62it/s]\n",
      "epoch : 130loss : 1.1871302127838135: 100%|██████████| 63/63 [00:04<00:00, 15.14it/s]\n",
      "epoch : 131loss : 1.174774408340454: 100%|██████████| 63/63 [00:04<00:00, 14.74it/s] \n",
      "epoch : 132loss : 1.1456501483917236: 100%|██████████| 63/63 [00:04<00:00, 14.89it/s]\n",
      "epoch : 133loss : 0.9434604644775391: 100%|██████████| 63/63 [00:04<00:00, 14.23it/s]\n",
      "epoch : 134loss : 0.8810089230537415: 100%|██████████| 63/63 [00:04<00:00, 14.44it/s]\n",
      "epoch : 135loss : 1.0050389766693115: 100%|██████████| 63/63 [00:04<00:00, 14.80it/s]\n",
      "epoch : 136loss : 1.046201467514038: 100%|██████████| 63/63 [00:04<00:00, 14.83it/s] \n",
      "epoch : 137loss : 1.0635502338409424: 100%|██████████| 63/63 [00:04<00:00, 14.75it/s]\n",
      "epoch : 138loss : 1.088322401046753: 100%|██████████| 63/63 [00:04<00:00, 14.75it/s] \n",
      "epoch : 139loss : 0.960741400718689: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s] \n",
      "epoch : 140loss : 1.050959825515747: 100%|██████████| 63/63 [00:04<00:00, 14.63it/s] \n",
      "epoch : 141loss : 0.9430721402168274: 100%|██████████| 63/63 [00:04<00:00, 14.70it/s]\n",
      "epoch : 142loss : 0.9098957180976868: 100%|██████████| 63/63 [00:04<00:00, 14.34it/s]\n",
      "epoch : 143loss : 0.8439114689826965: 100%|██████████| 63/63 [00:04<00:00, 14.63it/s]\n",
      "epoch : 144loss : 0.8352920413017273: 100%|██████████| 63/63 [00:04<00:00, 14.83it/s]\n",
      "epoch : 145loss : 0.7442215085029602: 100%|██████████| 63/63 [00:04<00:00, 14.66it/s]\n",
      "epoch : 146loss : 1.0859159231185913: 100%|██████████| 63/63 [00:04<00:00, 14.39it/s]\n",
      "epoch : 147loss : 0.8922675848007202: 100%|██████████| 63/63 [00:04<00:00, 14.58it/s]\n",
      "epoch : 148loss : 0.6983970403671265: 100%|██████████| 63/63 [00:04<00:00, 12.98it/s]\n",
      "epoch : 149loss : 0.7428358793258667: 100%|██████████| 63/63 [00:05<00:00, 11.24it/s]\n",
      "epoch : 150loss : 0.7136744260787964: 100%|██████████| 63/63 [00:05<00:00, 12.54it/s]\n",
      "epoch : 151loss : 0.6550666093826294: 100%|██████████| 63/63 [00:05<00:00, 12.50it/s]\n",
      "epoch : 152loss : 0.5950287580490112: 100%|██████████| 63/63 [00:05<00:00, 12.41it/s]\n",
      "epoch : 153loss : 0.6374211311340332: 100%|██████████| 63/63 [00:04<00:00, 12.86it/s]\n",
      "epoch : 154loss : 0.7508823275566101: 100%|██████████| 63/63 [00:04<00:00, 12.83it/s]\n",
      "epoch : 155loss : 0.6192716360092163: 100%|██████████| 63/63 [00:04<00:00, 14.08it/s]\n",
      "epoch : 156loss : 0.5606212615966797: 100%|██████████| 63/63 [00:04<00:00, 14.11it/s]\n",
      "epoch : 157loss : 0.7353094220161438: 100%|██████████| 63/63 [00:04<00:00, 14.51it/s]\n",
      "epoch : 158loss : 0.6354321241378784: 100%|██████████| 63/63 [00:05<00:00, 10.71it/s]\n",
      "epoch : 159loss : 0.47418832778930664: 100%|██████████| 63/63 [00:06<00:00, 10.46it/s]\n",
      "epoch : 160loss : 0.7416990399360657: 100%|██████████| 63/63 [00:06<00:00,  9.99it/s]\n",
      "epoch : 161loss : 0.8286652565002441: 100%|██████████| 63/63 [00:06<00:00,  9.88it/s]\n",
      "epoch : 162loss : 0.646023690700531: 100%|██████████| 63/63 [00:06<00:00, 10.05it/s] \n",
      "epoch : 163loss : 0.6487782597541809: 100%|██████████| 63/63 [00:06<00:00,  9.57it/s]\n",
      "epoch : 164loss : 0.9172589182853699: 100%|██████████| 63/63 [00:06<00:00,  9.22it/s]\n",
      "epoch : 165loss : 0.5974663496017456: 100%|██████████| 63/63 [00:06<00:00,  9.17it/s]\n",
      "epoch : 166loss : 0.48313021659851074: 100%|██████████| 63/63 [00:06<00:00,  9.98it/s]\n",
      "epoch : 167loss : 0.5549405813217163: 100%|██████████| 63/63 [00:06<00:00,  9.90it/s]\n",
      "epoch : 168loss : 0.35067155957221985: 100%|██████████| 63/63 [00:06<00:00,  9.04it/s]\n",
      "epoch : 169loss : 0.42336440086364746: 100%|██████████| 63/63 [00:06<00:00,  9.04it/s]\n",
      "epoch : 170loss : 0.32198336720466614: 100%|██████████| 63/63 [00:07<00:00,  9.00it/s]\n",
      "epoch : 171loss : 0.36714479327201843: 100%|██████████| 63/63 [00:07<00:00,  8.83it/s]\n",
      "epoch : 172loss : 0.3811357021331787: 100%|██████████| 63/63 [00:07<00:00,  8.98it/s]\n",
      "epoch : 173loss : 0.34300535917282104: 100%|██████████| 63/63 [00:06<00:00,  9.03it/s]\n",
      "epoch : 174loss : 0.2947109341621399: 100%|██████████| 63/63 [00:06<00:00,  9.12it/s]\n",
      "epoch : 175loss : 0.2810867428779602: 100%|██████████| 63/63 [00:06<00:00,  9.15it/s]\n",
      "epoch : 176loss : 0.2575739920139313: 100%|██████████| 63/63 [00:06<00:00,  9.27it/s] \n",
      "epoch : 177loss : 0.33494406938552856: 100%|██████████| 63/63 [00:06<00:00,  9.73it/s]\n",
      "epoch : 178loss : 0.38373348116874695: 100%|██████████| 63/63 [00:06<00:00,  9.21it/s]\n",
      "epoch : 179loss : 0.2598142623901367: 100%|██████████| 63/63 [00:06<00:00,  9.27it/s]\n",
      "epoch : 180loss : 0.3012232184410095: 100%|██████████| 63/63 [00:07<00:00,  8.80it/s] \n",
      "epoch : 181loss : 0.3085315525531769: 100%|██████████| 63/63 [00:07<00:00,  8.76it/s]\n",
      "epoch : 182loss : 0.313628613948822: 100%|██████████| 63/63 [00:07<00:00,  8.93it/s] \n",
      "epoch : 183loss : 0.22469015419483185: 100%|██████████| 63/63 [00:07<00:00,  8.93it/s]\n",
      "epoch : 184loss : 0.2754918336868286: 100%|██████████| 63/63 [00:07<00:00,  8.66it/s]\n",
      "epoch : 185loss : 0.41839495301246643: 100%|██████████| 63/63 [00:07<00:00,  8.43it/s]\n",
      "epoch : 186loss : 0.2331894338130951: 100%|██████████| 63/63 [00:06<00:00,  9.43it/s] \n",
      "epoch : 187loss : 0.3131363093852997: 100%|██████████| 63/63 [00:07<00:00,  8.94it/s] \n",
      "epoch : 188loss : 0.33578500151634216: 100%|██████████| 63/63 [00:06<00:00,  9.33it/s]\n",
      "epoch : 189loss : 0.2426353245973587: 100%|██████████| 63/63 [00:06<00:00,  9.05it/s] \n",
      "epoch : 190loss : 0.2487800121307373: 100%|██████████| 63/63 [00:07<00:00,  8.97it/s]\n",
      "epoch : 191loss : 0.285469114780426: 100%|██████████| 63/63 [00:06<00:00,  9.01it/s]  \n",
      "epoch : 192loss : 0.368696391582489: 100%|██████████| 63/63 [00:06<00:00,  9.85it/s]  \n",
      "epoch : 193loss : 0.2613995671272278: 100%|██████████| 63/63 [00:05<00:00, 11.20it/s] \n",
      "epoch : 194loss : 0.2123522013425827: 100%|██████████| 63/63 [00:05<00:00, 11.11it/s] \n",
      "epoch : 195loss : 0.2186414748430252: 100%|██████████| 63/63 [00:05<00:00, 11.92it/s] \n",
      "epoch : 196loss : 0.27191397547721863: 100%|██████████| 63/63 [00:05<00:00, 12.44it/s]\n",
      "epoch : 197loss : 0.22813919186592102: 100%|██████████| 63/63 [00:05<00:00, 12.53it/s]\n",
      "epoch : 198loss : 0.2717404067516327: 100%|██████████| 63/63 [00:04<00:00, 12.62it/s] \n",
      "epoch : 199loss : 0.2085341215133667: 100%|██████████| 63/63 [00:04<00:00, 12.65it/s] \n",
      "epoch : 200loss : 0.21958789229393005: 100%|██████████| 63/63 [00:04<00:00, 12.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델...\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# GPU가 사용 가능한 환경이면 GPU를 쓰고, 아니면 CPU를 쓰겠다\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset,batch_size=64)\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data,label in loop:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype=torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch : {epoch+1}loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34088f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 문장을 예측\n",
    "# 입력문 장을 텐서로 변경 임베딩 벡터 Bow를 이용해서\n",
    "sample = 'i love'\n",
    "with torch.no_grad():\n",
    "    words = torch.tensor(\n",
    "        [dataset.BOW[w] for w in sample.split()], dtype=torch.long\n",
    "    ).to(device).unsqueeze(0)\n",
    "    # (2,) --> (batch,sq_len) (1,2)\n",
    "    output = model(words)\n",
    "    \n",
    "    # 출력은 단어의 개수만큼 len(BOW) (batch, len(BOW))\n",
    "    # 확률이 가장 높은 단어 찾기\n",
    "    predicted_index = torch.argmax(output,dim=1).item()\n",
    "\n",
    "    # 단어사전 BOW에서 인덱스에 해당하는 단어를 찾기\n",
    "    # 역 dict를 만들어서 찾기\n",
    "\n",
    "    reverse_bow = {value:key for key,value in dataset.BOW.items()}\n",
    "    predicted_word = reverse_bow[predicted_index]   \n",
    "    print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21766a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 → 값 찾는 법 ^\n",
    "# https://chatgpt.com/s/t_6903268b21fc819195c5a4dc440d3233"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
